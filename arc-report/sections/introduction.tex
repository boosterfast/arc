\section{Introduction}

% What is the problem?
% 1) Data is being generated at an accelerating rate:
%   - It is not feasible to store all data in a database.
% 2) Data is becoming more accessible to the public domain.
%   - This has caused an emergence of applications and new requirements.
%   - We are seeing applications that use different types of data.
%   - Current languages are too constrained to alone support these requirements.
% 3) Data is being generated by more devices, some of which are at the edge.
%   - It is not feasible to run all data processing in a cloud environment.
% 
% Requirements based on the problems
%   - Systems must be able to ...
%     1) Scale against the velocity with which data is generated
%       - Process data in a streaming mode "as it is being generated"
%     2) Scale against increasing data volumes
%       - Process data in a batch mode "after all data has been generated"
%     3) Scale against the complexity of new requirements in the application
%       - Process data of different types in a concise way
%     4) Scale against the heterogeneity of the execution environments
%       - Process data from different types of devices
%       - Process data at different types of devices
%       - Programs must be able to run on any type of device
%   - Programs should be ..
%     1) Easy to write (few lines of code)
%     2) Easy to run (deployment)
%     3) Easy to scale

The amount of data generated by the world is increasing at an alarming rate. Data is also becoming more accessible to the public domain. This has caused an emergence of applications in data science that must run in different environments that include consumer-grade hardware, large scale cloud centers, and resource-constrained edge devices. Streaming sensor data, graph social networks, relational tables of product information, and tensors of climate science image data are few examples of datasets that can scale to the point where they can no longer be managed by a single machine. As a result, distributed shared-nothing data-parallel systems have become the norm for data intensive computing. These systems are able to scale against increasing problem sizes by partitioning data and parallelising computation across machines. Distributed systems programming, in its barest form, is however known to be notoriously difficult.

Without proper abstraction, application developers must manage problems such as fault tolerance and coordination while considering tradeoffs in security and efficiency. To this end, distributed systems leverage high-level DSLs in the form of query languages, frameworks, and libraries, which are more friendly towards end-users. DSLs allow developers to focus on domain-specific problems, such as the development of algorithms, and to disregard engineering-related issues. Not only do DSLs lend themselves to improved ease of use, but also optimisation potential. DSLs in the form of intermediate languages have been adopted by multiple systems both as a solution to enable reuse by breaking the dependence between the program specification and its execution, and to enable target-independent optimisation.

Not only is data becoming more widespread, but also more complex. DSLs must be able to express algorithms over many different types of data. Examples of such DSLs include Keras\cite{Keras} for machine learning and serving, SQL for data management, Apache Beam\cite{Beam} for stream processing, and Cypher\cite{Cypher} for graph analytics. While these DSLs are highly optimised towards specific applications, users are met with problems when trying to combine fragments of different DSLs to solve more advanced problems. First, users must pay the price of serialisation and data movement costs between systems due to the lack of a common data format, and hardware resource contention due to the lack of a common scheduler. Second, users must manage the impedance mismatch in guarantees between each system such as consistency, availability, and security. Third, users must learn how to program with tools of different systems, which may have widely different syntax and semantics.

We believe there is a need for a common DSL, IR, and execution environment for respectively expressing, optimising, and executing computations over different high-level data types. The functional requirements that this DSL must support are as follows:

\begin{itemize}
  \item \textbf{Big Data Abstractions}: The DSL should be able to express algorithms over massive and rapidly growing collections of data. In particular, streams, tensors, frames, and graphs.
  \item \textbf{User-Defined Behavior}: The DSL should be able to express user-defined behavior. For example, user-defined functions and datatypes.
\end{itemize}

The non-functional requirements that the runtime system must satisfy are as follows:

\begin{itemize}
  \item \textbf{Fault Tolerance}: The system should be able to sustain machine failure with efficiency. That is, without restarting the whole computation.
\end{itemize}

% The need for real-time analytics to uncover the deeper meaning of live data has become more prominent in recent years. In both academia and industry, large scale machine learning systems are a prime focus of research. Such systems have been optimised to train opaque machine learning models of many parameters against large amounts of input data. As an example, GPT-3 by OpenAI is the hallmark of general pre-trained models for language prediction. GPT-3 holds a capacity of 175 billion parameters and was trained on a dataset which effectively amounts to the entire Internet.

% While the models and the supporting technologies behind them have been fine-tuned for accuracy and performance, less effort has been dedicated to their integration with \textit{real-time analytics}. For this reason, the leap from prototyping a machine learning model to deploying it as an online service for mission-critical decision making requires both major engineering effort and domain expertise. Examples of such services include anomaly detection, adaptive recommender systems, time series forecasting, and real-time monitoring. More generally, the requirements these services specifically pose are that they must operate continuously with tolerance to failure, adaptively with respect to concept drift and resource changes, and flexibly by incorporating business logic.

% To this end, data analysts resort to system-frameworks for data intensive computing which provide a means for writing applications oriented around specific abstract data types and operations. By being abstract, analysts are able to ask questions about data to a system without needing to know the how the system arrives at its conclusions. Modern representative examples of such frameworks include:

% \begin{itemize}
%     \item \textbf{TensorFlow} \cite{TensorFlow}, a framework for portable machine learning oriented around tensors and linear algebra.
%     \item \textbf{Apache Flink} \cite{Flink}, a framework for low-latency stream processing oriented around data streams and stateful event-based logic.
%     \item \textbf{Apache Spark} \cite{Spark}, a framework for high-throughput batch processing oriented around data frames and relational algebra.
%     \item \textbf{Apache Giraph} \cite{Giraph}, a framework for large scale graph processing oriented around vertex-centric computation.
%     \item \textbf{Ray RLib} \cite{Ray}, a framework for distributed reinforcement learning oriented around agents, environments, and policies.
% \end{itemize}
% 
% A typical end-to-end deep analytics pipeline combines traditional data processing stages with machine learning and therefore requires a system and framework that supports relational algebra, graph processing, and linear algebra components. There is no doubt a combination of the aforementioned frameworks could be used for such analytics. However, as of yet there exists no framework and system that bridges each of the programming models and workloads while also supporting the real-time aspect of live data. This problem forces developers to deal with two types of complexity. First, they have to learn how to use and integrate multiple frameworks which in the worst case are written in different programming languages. Second, they have to consider low-level details that concern how systems interact, such as state management and data transfer, to achieve good performance.
% 
% In the CDA group, we are addressing this problem by engineering a whole new infrastructure for Continuous Deep Analytics. This infrastructure includes 1) a middleware for building distributed systems\footnote{Website: https://github.com/kompics/kompact}, 2) a distributed system built on top of the middleware for scalable stream and batch processing \citeP{Arcon} \footnote{Website: https://github.com/cda-group/arcon}, and 3) a domain-specific language for data science integrated to the system for programming with multiple abstract data types \citeP{Arc} \footnote{Website: https://github.com/cda-group/arc}. CDA is an interdisciplinary project funded by SSF, driven by a world-leading team of researchers at RISE and KTH, and destined to finish in the summer of 2023. The project in addition includes research on the development of novel online machine learning algorithms to be deployed on the CDA infrastructure. Project goals and preliminary results can be read more about in our midterm report \cite{CDA-Midterm}.

% * What is the problem?
% * What is the vision? (ideal solution)
% * What is our approach? (The CDA system: Applications, Arc-Lang, Arc-MLIR, Arcon, Kompact)
% * What are the challenges (requirements)?

% * Research Questions and Hypotheses
% * Background + Related Work (Existing solutions)
% * Problem and Motivation (Why Arc-Lang?)
% * Applications (What can Arc-Lang do / not do?)
% * Design principles

% * Unique features (What makes Arc-Lang special?)
% * Common features (What does Arc-Lang borrow from other solutions?)
% * Example programs (How can you use the features to solve problems?)

% Describe the language: Syntax and semantics

% Describe the runtime
% * Channels
% * Tasks
% * Data structures
