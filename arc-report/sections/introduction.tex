\section{Introduction}

% What is the problem?
% % Data is becoming bigger and more widespread.
% * Data is getting more complicated
% * Analytics are getting more complicated
% * Need to run analytics on heterogeneous hardware

The amount of data generated by the world is increasing at an accelerating rate. Data is also becoming more accessible to the public domain. This has caused an emergence of applications in data science that must run in different environments that include consumer-grade hardware, large scale cloud centers, and resource-constrained edge devices. Streaming sensor data, graph social networks, relational tables of product information, and tensors of climate science image data are few examples of datasets that can scale to the point where they can no longer be managed by a single machine. As a result, distributed shared-nothing data-parallel systems have become the norm for data intensive computing. These systems are able to scale against increasing problem sizes by partitioning data and parallelising computation across machines. Distributed systems programming, in its barest form, is however known to be notoriously difficult.

Without proper abstraction, application developers must manage problems such as fault tolerance and coordination while considering tradeoffs in security and efficiency. To this end, distributed systems leverage high-level DSLs in the form of query languages, frameworks, and libraries, which are more friendly towards end-users. DSLs allow developers to focus on domain-specific problems, such as the development of algorithms, and to disregard engineering-related issues. Not only do DSLs lend themselves to improved ease of use, but also optimisation potential. DSLs in the form of intermediate languages have been adopted by multiple systems both as a solution to enable reuse by breaking the dependence between the program specification and its execution, and to enable target-independent optimisation.

% 
Not only is data becoming more widespread, but also more complex. DSLs must be able to express algorithms over many different types of data. Examples of such DSLs include Keras\cite{Keras} for machine learning and serving, SQL\cite{SQL} for data management, Apache Beam\cite{Beam} for stream processing, and Cypher\cite{Cypher} for graph analytics. While these DSLs are highly optimised towards specific applications, users are met with problems when trying to combine fragments of different DSLs to solve more advanced problems. First, users must pay the price of serialisation and data movement costs between systems due to the lack of a common data format, and hardware resource contention due to the lack of a common scheduler. Second, users must manage the impedance mismatch in guarantees between each system such as consistency, availability, and security. Third, users must learn how to program with tools of different systems, which may have widely different syntax and semantics.

We believe there is a need for a common DSL, IR, and execution environment for respectively expressing, optimising, and executing computations over different high-level data types. The functional requirements that this DSL must support are as follows:

\subsection{High-Level Data Abstractions}%
\label{sub:High-Level Data Abstractions}

The DSL should be able to express algorithms over heterogeneous data types. For example images, tables, and graphs.

\subsection{User-Defined Behavior}%
\label{sub:User-Defined Behavior}

The DSL should be able to express user-defined behavior. For example, user-defined functions and datatypes.

The non-functional requirements that the runtime system must satisfy are as follows:

\subsection{Fault Tolerance}.
\label{sub:Fault-Tolerence}
The system should be able to sustain machine failure with efficiency. That is, without restarting the whole computation.


% The need for real-time analytics to uncover the deeper meaning of live data has become more prominent in recent years. In both academia and industry, large scale machine learning systems are a prime focus of research. Such systems have been optimised to train opaque machine learning models of many parameters against large amounts of input data. As an example, GPT-3 by OpenAI is the hallmark of general pre-trained models for language prediction. GPT-3 holds a capacity of 175 billion parameters and was trained on a dataset which effectively amounts to the entire Internet.

% While the models and the supporting technologies behind them have been fine-tuned for accuracy and performance, less effort has been dedicated to their integration with \textit{real-time analytics}. For this reason, the leap from prototyping a machine learning model to deploying it as an online service for mission-critical decision making requires both major engineering effort and domain expertise. Examples of such services include anomaly detection, adaptive recommender systems, time series forecasting, and real-time monitoring. More generally, the requirements these services specifically pose are that they must operate continuously with tolerance to failure, adaptively with respect to concept drift and resource changes, and flexibly by incorporating business logic.

% To this end, data analysts resort to system-frameworks for data intensive computing which provide a means for writing applications oriented around specific abstract data types and operations. By being abstract, analysts are able to ask questions about data to a system without needing to know the how the system arrives at its conclusions. Modern representative examples of such frameworks include:

% \begin{itemize}
%     \item \textbf{TensorFlow} \cite{TensorFlow}, a framework for portable machine learning oriented around tensors and linear algebra.
%     \item \textbf{Apache Flink} \cite{Flink}, a framework for low-latency stream processing oriented around data streams and stateful event-based logic.
%     \item \textbf{Apache Spark} \cite{Spark}, a framework for high-throughput batch processing oriented around data frames and relational algebra.
%     \item \textbf{Apache Giraph} \cite{Giraph}, a framework for large scale graph processing oriented around vertex-centric computation.
%     \item \textbf{Ray RLib} \cite{Ray}, a framework for distributed reinforcement learning oriented around agents, environments, and policies.
% \end{itemize}
% 
% A typical end-to-end deep analytics pipeline combines traditional data processing stages with machine learning and therefore requires a system and framework that supports relational algebra, graph processing, and linear algebra components. There is no doubt a combination of the aforementioned frameworks could be used for such analytics. However, as of yet there exists no framework and system that bridges each of the programming models and workloads while also supporting the real-time aspect of live data. This problem forces developers to deal with two types of complexity. First, they have to learn how to use and integrate multiple frameworks which in the worst case are written in different programming languages. Second, they have to consider low-level details that concern how systems interact, such as state management and data transfer, to achieve good performance.
% 
% In the CDA group, we are addressing this problem by engineering a whole new infrastructure for Continuous Deep Analytics. This infrastructure includes 1) a middleware for building distributed systems\footnote{Website: https://github.com/kompics/kompact}, 2) a distributed system built on top of the middleware for scalable stream and batch processing \citeP{Arcon} \footnote{Website: https://github.com/cda-group/arcon}, and 3) a domain-specific language for data science integrated to the system for programming with multiple abstract data types \citeP{Arc} \footnote{Website: https://github.com/cda-group/arc}. CDA is an interdisciplinary project funded by SSF, driven by a world-leading team of researchers at RISE and KTH, and destined to finish in the summer of 2023. The project in addition includes research on the development of novel online machine learning algorithms to be deployed on the CDA infrastructure. Project goals and preliminary results can be read more about in our midterm report \cite{CDA-Midterm}.

% * What is the problem?
% * What is the vision? (ideal solution)
% * What is our approach? (The CDA system: Applications, Arc-Lang, Arc-MLIR, Arcon, Kompact)
% * What are the challenges (requirements)?

\section{Introduction to Arc-Lang}

% * Research Questions and Hypotheses
% * Background + Related Work (Existing solutions)
% * Problem and Motivation (Why Arc-Lang?)
% * Applications (What can Arc-Lang do / not do?)
% * Design principles

\subsection{Tour of Arc-Lang}

% * Unique features (What makes Arc-Lang special?)
% * Common features (What does Arc-Lang borrow from other solutions?)
% * Example programs (How can you use the features to solve problems?)

\subsection{High-Level Model}

% Describe the language: Syntax and semantics

\subsection{Execution Model}

% Describe the runtime
% * Channels
% * Tasks
% * Data structures
